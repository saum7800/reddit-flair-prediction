{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3B Embedding layer classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpxbFofoCELl",
        "colab_type": "text"
      },
      "source": [
        "In this notebook, we will be using a pre trained word emedding layer provided by Google on tensorflow Hub. This will be followed by 2 dense layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncZvNqYe7jfM",
        "colab_type": "code",
        "outputId": "53ed0785-d540-42d0-f219-60546a46b17c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "!pip install tensorflow-hub\n",
        "\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.18.3)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (46.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFIaL1awCv4R",
        "colab_type": "text"
      },
      "source": [
        "Let us import our dataset and remove the one NULL data point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEau4pXs7tsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "df = pd.read_csv('drive/My Drive/dataset_final.csv', engine='python')\n",
        "df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
        "sr = pd.isnull(df['title'])\n",
        "sr.loc[sr==True]\n",
        "df.drop(83369, axis=0, inplace=True)\n",
        "df = df.reset_index(drop=True)\n",
        "sr = pd.isnull(df['full_text'])\n",
        "drop_arr = sr.loc[sr==True].index.tolist()\n",
        "df.drop(drop_arr, axis=0, inplace=True)\n",
        "df = df.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8PpUAVNDFkA",
        "colab_type": "text"
      },
      "source": [
        "We will extract our full text and flair columns and split the data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdBl035Qy8Dd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_flairs = df['flair'].index.tolist()\n",
        "X_text = list(df['full_text'])\n",
        "Y = list(df['flair'])\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_text, Y, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKMY0GP3EZt4",
        "colab_type": "text"
      },
      "source": [
        "We must convert the format of our output into one hot vectors that tensorflow can understand"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX2_yYtW8M0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "encoder = LabelBinarizer()\n",
        "encoder.fit(y_train)\n",
        "y_train_bin = encoder.transform(y_train)\n",
        "y_test_bin = encoder.transform(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en5MSbloEjjn",
        "colab_type": "text"
      },
      "source": [
        "We can utilize class weights to perform oversampling directly in tensorflow. The model in this notebook does not use it eventually as it ended up giving worse results than the model without using any class weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cg7xQyxqyi-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight('balanced' ,encoder.classes_, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mBzD1f1FGrh",
        "colab_type": "text"
      },
      "source": [
        "We will now build and compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Urifttz8oAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1\"\n",
        "hub_layer = hub.KerasLayer(embedding, input_shape=[],\n",
        "                           dtype=tf.string, trainable=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1K-cd7J9edT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(hub_layer)\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(10))\n",
        "model.add(tf.keras.layers.Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4dbI9n_9xhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CocBpQodJg8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_tensor = tf.convert_to_tensor(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFe9SuVd3E1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_weights = {i:class_weights[i] for i in range(len(class_weights))}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20BxuhZC-ehu",
        "colab_type": "code",
        "outputId": "fe779b33-bf24-4d87-c6c1-d26db4095f37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "model.fit(x=x_train_tensor,y=y_train_bin, batch_size=128, epochs=10, verbose=1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "580/580 [==============================] - 3s 5ms/step - loss: 1.0912 - accuracy: 0.6362\n",
            "Epoch 2/10\n",
            "580/580 [==============================] - 3s 5ms/step - loss: 0.8877 - accuracy: 0.6984\n",
            "Epoch 3/10\n",
            "580/580 [==============================] - 3s 5ms/step - loss: 0.8408 - accuracy: 0.7127\n",
            "Epoch 4/10\n",
            "580/580 [==============================] - 3s 5ms/step - loss: 0.8068 - accuracy: 0.7241\n",
            "Epoch 5/10\n",
            "580/580 [==============================] - 3s 5ms/step - loss: 0.7805 - accuracy: 0.7323\n",
            "Epoch 6/10\n",
            "580/580 [==============================] - 3s 5ms/step - loss: 0.7557 - accuracy: 0.7392\n",
            "Epoch 7/10\n",
            "580/580 [==============================] - 3s 5ms/step - loss: 0.7305 - accuracy: 0.7503\n",
            "Epoch 8/10\n",
            "580/580 [==============================] - 3s 5ms/step - loss: 0.7090 - accuracy: 0.7574\n",
            "Epoch 9/10\n",
            "580/580 [==============================] - 3s 5ms/step - loss: 0.6877 - accuracy: 0.7643\n",
            "Epoch 10/10\n",
            "580/580 [==============================] - 3s 5ms/step - loss: 0.6671 - accuracy: 0.7713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f487cd6b5f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gi_wNVjMI_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test_tensor = tf.convert_to_tensor(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRYRY74v--0K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b2674d2d-c255-43b4-a6bb-918eb7b3bc93"
      },
      "source": [
        "model.evaluate(x_test_tensor, y_test_bin)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "994/994 [==============================] - 3s 3ms/step - loss: 1.2526 - accuracy: 0.5853\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.2525954246520996, 0.5852799415588379]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLFFs7LrqJzA",
        "colab_type": "text"
      },
      "source": [
        "We see that this result is better than the one we got from using the RNN from "
      ]
    }
  ]
}