{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "Before collecting any data, we must answer several questions about how we wish to gather our data and what constitutes a good dataset. \n",
    "\n",
    " 1. **What is the time period we collect our data from?** <br>\n",
    "\tA very **narrow time period** such as the last few months would be a **very biased representation** of the subreddit as there would be major events we would miss any posts on and more importantly bias the dataset to focus on a few certain events too much. For example, if we collected the data of the last four months, all future analysis and models would give extreme weightage to coronavirus related posts which is obviously not a balanced representation of the subreddit. The state of the subreddit **10 years ago** would be extremely different due to the **data and mobile revolution** since then. This is why an appropriate time frame would be posts from the **last four years, i.e. 2016-2020** hence covering all major recent events.\n",
    "<br><br>\n",
    "\n",
    "\n",
    " 2. **How much data should we collect?** <br>\n",
    "\tThis is purely dependent on the **resources available** to us for data exploration and model training. A reasonable number of posts I can successfully train on my PC is around **1,50,000**. This number when combined with the time frame of last four years would average around **100 posts per day**.\n",
    "<br><br>\n",
    "\n",
    "3. **What is the method for collecting posts?** <br>\n",
    "\tThere are several ways in which we can form our dataset such as:\n",
    "\t\n",
    "\t\t1. equal number of posts from each popular flair\n",
    "\t\t2. posts with the highest score\n",
    "\t\t3. random posts\n",
    "\tAfter pondering over various methods, I realized that I wanted to strike a balance between  **relevant posts** and a good distribution of posts over time. I define a relevant post to be one which is viewed and liked by many people hence displaying it's impact on society. For this reason, I divided the four year period into batches of **10 days each**. For each batch, I would collect the **top 1000 liked posts**. Although this would make for an imbalanced dataset in terms of flairs, it is an accurate representation of the relevant posts on the subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method for getting posts\n",
    "\n",
    "There are two widely used methods for getting posts: \n",
    "1. **PRAW Reddit API wrapper**<br>\n",
    "    In the latest version of the API, one can only get all the post from one of three categories: hot, new and top. This restricts collection of posts to a total of 1000 posts from any categories. This means that even if we consider no overlap between posts in different categories, I can get only a maximum of 3000 posts. This will not work.\n",
    "<br><br>\n",
    "2. **pushshift API**<br>\n",
    "    This API allows sending a GET request to get a maximum of 1000 posts in one request. You can set various parameters in the request to specify a time period in which the posts must be and sort them according to any parameter. We will make use of this.\n",
    "<br><br>\n",
    "We request for top 1000 scoring posts for a time frame between ```(epoch - ten_days, epoch)``` 144 times to cover the entire 4 year time span\n",
    "\n",
    "Note: If you wish to replicate the results further, you may execute the following code blocks but for the data exploration notebook, use the given reddit_data.csv and not the one you would generate here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n"
     ]
    }
   ],
   "source": [
    "start_timestamp = int(datetime.utcnow().timestamp())\n",
    "url = \"https://api.pushshift.io/reddit/submission/search/?score=>0&before={}&after={}&sort_type=score&sort=desc&subreddit=india&limit=1000\"\n",
    "dataset = []\n",
    "epoch = start_timestamp\n",
    "ten_days = 10*24*60*60\n",
    "epoch_prev = epoch - ten_days\n",
    "post_counts = 0\n",
    "total_years = 4\n",
    "for every_ten_days in range(total_years*12*3):\n",
    "    final_url = url.format(str(epoch),str(epoch_prev))\n",
    "    json_data = requests.get(final_url, headers={'User-Agent': \"test reddit app\"})\n",
    "    data = json_data.json()\n",
    "    posts = data['data']\n",
    "    for post in posts:\n",
    "        post_counts = post_counts + 1\n",
    "        update_array = get_update_array(post)\n",
    "        dataset.append(update_array)\n",
    "    epoch = epoch_prev\n",
    "    epoch_prev = epoch - ten_days\n",
    "    print(i)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```def get_update_array(post)``` takes as parameter a dictionary which represents information about a post. The function then extracts the important information from the post required for data exploration and model training and returns it as a list to be appended to the final dataset \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_update_array(post):\n",
    "    title = post['title']\n",
    "    author = post['author']\n",
    "    created_utc = post['created_utc']\n",
    "    self_post = post['is_self']\n",
    "    score = post['score']\n",
    "    over_18 = post['over_18']\n",
    "    num_comments = post['num_comments']\n",
    "    \n",
    "    if 'is_original_content' not in post:\n",
    "        is_original_content = None\n",
    "    else:\n",
    "        is_original_content = post['is_original_content']\n",
    "    \n",
    "    if 'selftext' not in post:\n",
    "        self_text = \"\"\n",
    "    else:\n",
    "        self_text = post['selftext']\n",
    "    \n",
    "    if 'link_flair_text' not in post:\n",
    "        flair = None\n",
    "    else:\n",
    "        flair = post['link_flair_text']\n",
    "    \n",
    "    update_array = [title, flair, score, num_comments, author, is_original_content, created_utc, self_post, self_text, over_18]\n",
    "    return update_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert our list to a pandas dataframe which will form the basis of our data exploration and convert it to CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>flair</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>author</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>self_post</th>\n",
       "      <th>self_text</th>\n",
       "      <th>over_18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Even the poorest are supporting Modi in this.</td>\n",
       "      <td>Politics</td>\n",
       "      <td>64</td>\n",
       "      <td>78</td>\n",
       "      <td>hungarywolf</td>\n",
       "      <td>False</td>\n",
       "      <td>1586106160</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Someone tried to sell Statue of Unity on Olx. ...</td>\n",
       "      <td>Non-Political</td>\n",
       "      <td>50</td>\n",
       "      <td>143</td>\n",
       "      <td>Athar147</td>\n",
       "      <td>False</td>\n",
       "      <td>1586105586</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Captured India with my phone.</td>\n",
       "      <td>Photography</td>\n",
       "      <td>49</td>\n",
       "      <td>202</td>\n",
       "      <td>random_saiyajin</td>\n",
       "      <td>False</td>\n",
       "      <td>1586102173</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Irony</td>\n",
       "      <td>None</td>\n",
       "      <td>43</td>\n",
       "      <td>109</td>\n",
       "      <td>blue_mark</td>\n",
       "      <td>False</td>\n",
       "      <td>1586100822</td>\n",
       "      <td>True</td>\n",
       "      <td>My entire house has it's lights turned off and...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You guys are too impure to understand Modi Ji'...</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>39</td>\n",
       "      <td>81</td>\n",
       "      <td>AdmiralSP</td>\n",
       "      <td>False</td>\n",
       "      <td>1586100637</td>\n",
       "      <td>True</td>\n",
       "      <td>It's very scientific.\\nI'm a scientist.\\n\\nEve...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BSF soldier lights candles in his bunker.</td>\n",
       "      <td>| Unverified Content / Disreputed Source |</td>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "      <td>zxkool</td>\n",
       "      <td>False</td>\n",
       "      <td>1586106901</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Posting again because stupidity was on show to...</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>34</td>\n",
       "      <td>21</td>\n",
       "      <td>msbuttergourd</td>\n",
       "      <td>False</td>\n",
       "      <td>1586108196</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>During #9PM9minute All India demand of electri...</td>\n",
       "      <td>Non-Political</td>\n",
       "      <td>32</td>\n",
       "      <td>107</td>\n",
       "      <td>The_andh_bhakth</td>\n",
       "      <td>False</td>\n",
       "      <td>1586114141</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fuck this. Our country will never learn.</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>32</td>\n",
       "      <td>349</td>\n",
       "      <td>youdidWHaAtnow</td>\n",
       "      <td>False</td>\n",
       "      <td>1586101383</td>\n",
       "      <td>True</td>\n",
       "      <td>People are bursting crackers. They're playing ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Antila World's most expensive home - lights of...</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>31</td>\n",
       "      <td>61</td>\n",
       "      <td>neilupinto</td>\n",
       "      <td>False</td>\n",
       "      <td>1586105379</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0      Even the poorest are supporting Modi in this.   \n",
       "1  Someone tried to sell Statue of Unity on Olx. ...   \n",
       "2                      Captured India with my phone.   \n",
       "3                                          The Irony   \n",
       "4  You guys are too impure to understand Modi Ji'...   \n",
       "5          BSF soldier lights candles in his bunker.   \n",
       "6  Posting again because stupidity was on show to...   \n",
       "7  During #9PM9minute All India demand of electri...   \n",
       "8           Fuck this. Our country will never learn.   \n",
       "9  Antila World's most expensive home - lights of...   \n",
       "\n",
       "                                        flair  score  num_comments  \\\n",
       "0                                    Politics     64            78   \n",
       "1                               Non-Political     50           143   \n",
       "2                                 Photography     49           202   \n",
       "3                                        None     43           109   \n",
       "4                                 Coronavirus     39            81   \n",
       "5  | Unverified Content / Disreputed Source |     35             4   \n",
       "6                                 Coronavirus     34            21   \n",
       "7                               Non-Political     32           107   \n",
       "8                                 Coronavirus     32           349   \n",
       "9                                 Coronavirus     31            61   \n",
       "\n",
       "            author is_original_content  created_utc  self_post  \\\n",
       "0      hungarywolf               False   1586106160      False   \n",
       "1         Athar147               False   1586105586      False   \n",
       "2  random_saiyajin               False   1586102173      False   \n",
       "3        blue_mark               False   1586100822       True   \n",
       "4        AdmiralSP               False   1586100637       True   \n",
       "5           zxkool               False   1586106901      False   \n",
       "6    msbuttergourd               False   1586108196      False   \n",
       "7  The_andh_bhakth               False   1586114141      False   \n",
       "8   youdidWHaAtnow               False   1586101383       True   \n",
       "9       neilupinto               False   1586105379      False   \n",
       "\n",
       "                                           self_text  over_18  \n",
       "0                                                       False  \n",
       "1                                                       False  \n",
       "2                                                       False  \n",
       "3  My entire house has it's lights turned off and...    False  \n",
       "4  It's very scientific.\\nI'm a scientist.\\n\\nEve...    False  \n",
       "5                                                       False  \n",
       "6                                                       False  \n",
       "7                                                       False  \n",
       "8  People are bursting crackers. They're playing ...    False  \n",
       "9                                                       False  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dataset, columns =['title', 'flair', 'score', 'num_comments', 'author', 'is_original_content', 'created_utc', 'self_post', 'self_text', 'over_18'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('reddit_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
